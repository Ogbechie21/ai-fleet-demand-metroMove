{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6de65d3",
   "metadata": {},
   "source": [
    "# Fleet Analytics & Prediction System\n",
    "\n",
    "Author: Christopher F. Ogbechie , Lukman Ibrahim and Mathew Asare\n",
    "        \n",
    "Course: ANLT202  \n",
    "\n",
    "## Project Objectives\n",
    "This notebook prepares the dataset for:\n",
    "- Trip delay risk classification\n",
    "- Maintenance cost regression modeling\n",
    "\n",
    "## Dataset\n",
    "Source: Kaggle – Fleet Dataset  \n",
    "File: fleet_dummy_5000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314216c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49836b34",
   "metadata": {},
   "source": [
    "## Classification Task\n",
    "Predict whether a trip is operationally risky or problematic.\n",
    "\n",
    "We define a trip as High Risk (1) if the status indicates a delay or problem\n",
    "(e.g., \"Delayed\", \"Cancelled\", \"Failed\"). Otherwise, it is Low Risk (0).\n",
    "\n",
    "This helps the fleet manager identify trips likely to cause service issues.\n",
    "\n",
    "## Regression Task\n",
    "Predict the maintenance_cost of a vehicle/trip based on:\n",
    "- distance_km\n",
    "- vehicle age / type\n",
    "- driving behaviour (violations, speeding incidents)\n",
    "- fuel_cost, toll_cost, load_value\n",
    "- weather_condition and route information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b99556",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/fleet_dummy_5000.csv\")# Load the fleet dataset\n",
    "\n",
    "df.head() # Preview the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf48285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"status\"].value_counts() # Count how many trips fall under each status category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b621c",
   "metadata": {},
   "source": [
    "## Define Classification Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize status to lowercase\n",
    "df[\"status_lower\"] = df[\"status\"].str.lower()\n",
    "\n",
    "# Define high risk trips (only Delayed)\n",
    "df[\"high_risk\"] = (df[\"status_lower\"] == \"delayed\").astype(int)\n",
    "\n",
    "# Check result\n",
    "df[\"high_risk\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8973f",
   "metadata": {},
   "source": [
    "## Define Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_time to datetime\n",
    "df[\"pickup_time\"] = pd.to_datetime(df[\"pickup_time\"])\n",
    "\n",
    "# Add time-based features\n",
    "df[\"pickup_hour\"] = df[\"pickup_time\"].dt.hour\n",
    "df[\"pickup_dayofweek\"] = df[\"pickup_time\"].dt.dayofweek\n",
    "\n",
    "# Feature columns\n",
    "features = [\n",
    "    \"distance_km\",\n",
    "    \"fuel_cost\",\n",
    "    \"driver_pay\",\n",
    "    \"toll_cost\",\n",
    "    \"load_value\",\n",
    "    \"violation_count\",\n",
    "    \"speeding_incidents\",\n",
    "    \"gps_start_lat\",\n",
    "    \"gps_start_lon\",\n",
    "    \"gps_end_lat\",\n",
    "    \"gps_end_lon\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dayofweek\"\n",
    "]\n",
    "\n",
    "# Feature matrix\n",
    "X = df[features]\n",
    "\n",
    "# Targets\n",
    "y_class = df[\"high_risk\"]          # classification\n",
    "y_reg = df[\"maintenance_cost\"]     # regression\n",
    "\n",
    "X.head(), y_class.head(), y_reg.head() # Quickly preview the features and both target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8a8e3",
   "metadata": {},
   "source": [
    "## We checked dataset shapes and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of our feature set and target variables\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_class shape:\", y_class.shape)\n",
    "print(\"y_reg shape:\", y_reg.shape)\n",
    "\n",
    "# Check for missing values in selected columns\n",
    "df[features + [\"maintenance_cost\", \"high_risk\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebeeaa1",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd228a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data for the classification task (predicting high-risk trips)\n",
    "# We use stratify so the class balance stays the same in train and test sets.\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Split data for the regression task (predicting maintenance cost)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "# Show the shapes to confirm splits happened correctly\n",
    "print(\"Classification Train:\", X_train_c.shape, \"Test:\", X_test_c.shape)\n",
    "print(\"Regression Train:\", X_train_r.shape, \"Test:\", X_test_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ced5e",
   "metadata": {},
   "source": [
    "## List all numeric feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd633ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all numeric feature columns\n",
    "numeric_features = X.columns.tolist()\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4ea56",
   "metadata": {},
   "source": [
    "## Scaling & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "preprocess_numeric = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498422e",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # View summary statistics for numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1c4d1",
   "metadata": {},
   "source": [
    "## Classification Models & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db6e94",
   "metadata": {},
   "source": [
    "## Building the Logistic Regression Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2070de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Build a pipeline that scales the data and trains a Logistic Regression model\n",
    "clf_logreg = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=1000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the model on the classification training data\n",
    "clf_logreg.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_logreg = clf_logreg.predict(X_test_c)\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"Logistic Regression Results:\\n\")\n",
    "print(classification_report(y_test_c, y_pred_logreg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_c, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508ca28",
   "metadata": {},
   "source": [
    "## Building the Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14be17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build a Random Forest pipeline (no scaling needed for tree models)\n",
    "clf_rf = Pipeline(\n",
    "    steps=[\n",
    "        # Tree models don't strictly need scaling, so we skip StandardScaler here\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the Random Forest on the classification data\n",
    "clf_rf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = clf_rf.predict(X_test_c)\n",
    "\n",
    "# Show performance results\n",
    "print(\"=== Random Forest Results ===\\n\")\n",
    "print(classification_report(y_test_c, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_c, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e509cf7",
   "metadata": {},
   "source": [
    "## Balanced Logistic Regression Model (Handling Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with class weights to handle class imbalance\n",
    "clf_logreg_bal = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "# Train the balanced model\n",
    "clf_logreg_bal.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_logreg_bal = clf_logreg_bal.predict(X_test_c)\n",
    "\n",
    "# Show performance results\n",
    "print(\"=== Balanced Logistic Regression Results ===\\n\")\n",
    "print(classification_report(y_test_c, y_pred_logreg_bal))\n",
    "print(confusion_matrix(y_test_c, y_pred_logreg_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956c191",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with class weights to better handle imbalanced classes\n",
    "clf_rf_bal = Pipeline(\n",
    "    steps=[\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced\"\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "# Train the balanced Random Forest model\n",
    "clf_rf_bal.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf_bal = clf_rf_bal.predict(X_test_c)\n",
    "\n",
    "# Show evaluation results\n",
    "print(\"=== Balanced Random Forest Results ===\\n\")\n",
    "print(classification_report(y_test_c, y_pred_rf_bal))\n",
    "print(confusion_matrix(y_test_c, y_pred_rf_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a943e",
   "metadata": {},
   "source": [
    "##  Regression Models & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f73bb",
   "metadata": {},
   "source": [
    "## Linear Regression Model (Fuel Consumption Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80257e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Build a pipeline that scales the data and fits a Linear Regression model\n",
    "reg_lin = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the regression model\n",
    "reg_lin.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lin = reg_lin.predict(X_test_r)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test_r, y_pred_lin) # Average error\n",
    "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_lin)) # Root MSE\n",
    "r2 = r2_score(y_test_r, y_pred_lin) # Variation explained\n",
    "\n",
    "# Display results\n",
    "print(\"=== Linear Regression Results ===\")\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48893b9f",
   "metadata": {},
   "source": [
    "## Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Build a Random Forest model inside a pipeline\n",
    "reg_rf = Pipeline([\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=200, # Number of trees\n",
    "        random_state=42   # Keep results consistent\n",
    "    ))\n",
    "])\n",
    "# Train the model on the regression training data\n",
    "reg_rf.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Predict fuel/maintenance cost on the test set\n",
    "y_pred_rf = reg_rf.predict(X_test_r)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae_rf = mean_absolute_error(y_test_r, y_pred_rf) # Average prediction error\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test_r, y_pred_rf)) # Root mean squared error\n",
    "r2_rf = r2_score(y_test_r, y_pred_rf) # How much variance the model explains\n",
    "\n",
    "# Display model performance\n",
    "print(\"=== Random Forest Regressor Results ===\")\n",
    "print(\"MAE:\", mae_rf)\n",
    "print(\"RMSE:\", rmse_rf)\n",
    "print(\"R²:\", r2_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9f7f2",
   "metadata": {},
   "source": [
    "## Feature Selection for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the feature columns we want to use for the regression model\n",
    "reg_feature_cols = [\n",
    "    \"distance_km\",\n",
    "    \"fuel_cost\",\n",
    "    \"driver_pay\",\n",
    "    \"toll_cost\",\n",
    "    \"load_value\",\n",
    "    \"violation_count\",\n",
    "    \"speeding_incidents\",\n",
    "    \"gps_start_lat\",\n",
    "    \"gps_start_lon\",\n",
    "    \"gps_end_lat\",\n",
    "    \"gps_end_lon\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dayofweek\"\n",
    "]\n",
    "\n",
    "# Build the regression feature matrix using only the selected columns\n",
    "X_reg = df[reg_feature_cols]\n",
    "\n",
    "# Split data again (specifically for the regression task)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "# Print shape to confirm everything looks correct\n",
    "print(X_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0f420",
   "metadata": {},
   "source": [
    "## Linear Regression Model (Fuel Consumption Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline that scales the data and applies Linear Regression\n",
    "reg_lin = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "# Train the model\n",
    "reg_lin.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lin = reg_lin.predict(X_test_r)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test_r, y_pred_lin)  # Average error\n",
    "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_lin)) # Root mean squared error\n",
    "r2 = r2_score(y_test_r, y_pred_lin)  # How well the model explains variation\n",
    "\n",
    "# Display results\n",
    "print(\"=== Fixed Linear Regression Results ===\")\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c804f5",
   "metadata": {},
   "source": [
    "## Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4848be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline that fits a Random Forest Regressor\n",
    "reg_rf = Pipeline([\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=200, # Number of trees in the forest\n",
    "        random_state=42 # Keeps results consistent\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the Random Forest model\n",
    "reg_rf.fit(X_train_r, y_train_r)\n",
    "# Predict on the regression test set\n",
    "y_pred_rf = reg_rf.predict(X_test_r)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae_rf = mean_absolute_error(y_test_r, y_pred_rf) # Average prediction error\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test_r, y_pred_rf)) # Root mean squared error\n",
    "r2_rf = r2_score(y_test_r, y_pred_rf) # % of variance explained by the model\n",
    "\n",
    "# Show model performance results\n",
    "print(\"=== Fixed Random Forest Results ===\")\n",
    "print(\"MAE:\", mae_rf)\n",
    "print(\"RMSE:\", rmse_rf)\n",
    "print(\"R²:\", r2_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53c331",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Search for the best Logistic Regression settings\n",
    "param_grid_logreg = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10], # Strength of regularization\n",
    "    \"model__solver\": [\"lbfgs\", \"liblinear\"] # Optimization methods\n",
    "}\n",
    "\n",
    "# GridSearchCV tries every combination to find the best model\n",
    "grid_logreg = GridSearchCV(\n",
    "    clf_logreg_bal,    # Balanced Logistic Regression model\n",
    "    param_grid_logreg,  # Parameters to test\n",
    "    scoring=\"recall\",   # We focus on catching delayed trips\n",
    "    cv=5,               # 5-fold cross validation\n",
    "    n_jobs=-1           # Use all CPU cores\n",
    ")\n",
    "# Train the tuning search\n",
    "grid_logreg.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Print the best settings found\n",
    "print(\"Best Logistic Regression Parameters:\", grid_logreg.best_params_)\n",
    "print(\"Best Cross-Validated Recall:\", grid_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31bcde",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5687163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings we want GridSearchCV to test for the Random Forest model\n",
    "param_grid_rf = {\n",
    "    \"model__n_estimators\": [50, 100, 200],  # Number of trees\n",
    "    \"model__max_depth\": [None, 10, 20],     # How deep each tree can grow\n",
    "}\n",
    "# GridSearchCV tries different parameter combinations to find the best model\n",
    "grid_rf = GridSearchCV(\n",
    "    reg_rf,     # Our Random Forest pipeline\n",
    "    param_grid_rf,  # Parameter grid to search\n",
    "    scoring=\"neg_mean_absolute_error\",  # We want the lowest MAE\n",
    "    cv=5,                       # 5-fold cross validation\n",
    "    n_jobs=-1       # Use all CPU cores\n",
    ")\n",
    "# Run the tuning process\n",
    "grid_rf.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Print the best parameters and the best MAE score found\n",
    "print(\"Best Random Forest Parameters:\", grid_rf.best_params_)\n",
    "print(\"Best CV MAE:\", -grid_rf.best_score_) # Convert negative MAE back to positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cbc51",
   "metadata": {},
   "source": [
    "## Data Scaling for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep learning models work better when inputs are scaled\n",
    "scaler_dl = StandardScaler()\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_dl = scaler_dl.fit_transform(X_train_c)\n",
    "# Apply the same scaling to the test data\n",
    "X_test_dl = scaler_dl.transform(X_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52d284",
   "metadata": {},
   "source": [
    "## Building the Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe50ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build a simple neural network for classification\n",
    "model = Sequential()\n",
    "# First hidden layer with 32 neurons\n",
    "model.add(Dense(32, activation=\"relu\", input_shape=(X_train_dl.shape[1],)))\n",
    "# Second hidden layer with 16 neurons\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "# Output layer with 1 neuron for binary classification\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Show the model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe5b90",
   "metadata": {},
   "source": [
    "## Training the ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network model\n",
    "history = model.fit(\n",
    "    # Training data\n",
    "    X_train_dl, y_train_c,\n",
    "    # Validation during training\n",
    "    validation_data=(X_test_dl, y_test_c),\n",
    "    # Number of passes through the data\n",
    "    epochs=30,\n",
    "    # Samples per training batch\n",
    "    batch_size=32,\n",
    "    # Show training progress\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the ANN model (convert probabilities to 0/1 classes)\n",
    "y_pred_dl = (model.predict(X_test_dl) > 0.5).astype(int).flatten()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Display evaluation results for the deep learning model\n",
    "print(\"=== Deep Learning Classification Results ===\")\n",
    "print(classification_report(y_test_c, y_pred_dl)) # Precision, recall, F1-score\n",
    "print(confusion_matrix(y_test_c, y_pred_dl)) # Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f9c83",
   "metadata": {},
   "source": [
    "## Evaluating the ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94034a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Find the unique classes (0 = on-time, 1 = delayed)\n",
    "classes = np.unique(y_train_c)\n",
    "# Compute balanced weights so the model treats both classes fairly\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_c\n",
    ")\n",
    "# Put the weights into a dictionary format used by Keras\n",
    "class_weight_dict = {int(c): w for c, w in zip(classes, class_weights)}\n",
    "# Show the class weights\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab75589",
   "metadata": {},
   "source": [
    "## Training the ANN Model with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ANN using class weights to handle imbalance\n",
    "history = model.fit(\n",
    "    # Training data\n",
    "    X_train_dl, y_train_c,\n",
    "    # Validation set\n",
    "    validation_data=(X_test_dl, y_test_c),\n",
    "    # Number of training cycles\n",
    "    epochs=30,\n",
    "    # Size of each training batch\n",
    "    batch_size=32,\n",
    "    # Apply class balancing\n",
    "    class_weight=class_weight_dict,\n",
    "    # Show training progress\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe597d",
   "metadata": {},
   "source": [
    "## Train–Test Split Confirmation (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the size of the training and test sets for classification\n",
    "print(\"Classification Train:\", X_train_c.shape, \"Test:\", X_test_c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
